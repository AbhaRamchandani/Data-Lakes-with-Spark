{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "\n",
    "# more imports for converting ts to timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import to_timestamp, monotonically_increasing_id\n",
    "\n",
    "# more imports for counting records\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID'] = config.get('USER', 'AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config.get('USER', 'AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "#print(os.environ['AWS_ACCESS_KEY_ID'])\n",
    "#print(os.environ['AWS_SECRET_ACCESS_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "#spark.sparkContext.master\n",
    "\n",
    "#spark.udf.register('noop', lambda x: x)\n",
    "\n",
    "# specify data locations - 'Udacity's json input' and 'Student's parquet output'\n",
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://project4-spark/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: get filepath to song data file\n",
    "# only a subset of the data considered\n",
    "song_data = input_data + \"song-data/A/A/A/*.json\"\n",
    "\n",
    "#print(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: read song data file\n",
    "df = spark.read.json(song_data)\n",
    "\n",
    "#print(df)\n",
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: extract columns to create songs table\n",
    "songs_table = df.select('song_id', 'artist_id', 'year', 'duration')\n",
    "\n",
    "#print(songs_table)\n",
    "#songs_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.partitionBy('year', 'artist_id').parquet(output_data + \"songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: extract columns to create artists table\n",
    "artists_table = df.select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude')\n",
    "\n",
    "#print(artists_table)\n",
    "#artists_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: write artists table to parquet files\n",
    "artists_table.write.parquet(output_data + \"artists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: get filepath to log data file\n",
    "log_data = input_data + 'log-data/*/*/*.json'\n",
    "\n",
    "#print(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: read log data file\n",
    "df = spark.read.json(log_data)\n",
    "\n",
    "print(df)\n",
    "df.show(5)\n",
    "#print(df.shape[0]) -> AttributeError: 'DataFrame' object has no attribute 'shape'\n",
    "#print(len(df)) -> TypeError: object of type 'DataFrame' has no len()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: filter by actions for song plays\n",
    "df = df.filter(col(\"page\") == 'NextSong')\n",
    "\n",
    "print(df)\n",
    "df.show(5)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: extract columns for users table    \n",
    "# changed table name from artists_table to users_table to reflect the purpose\n",
    "users_table = df.select(col(\"userId\").alias(\"user_id\"),col(\"firstName\").alias(\"first_name\"), col(\"lastName\").alias(\"last_name\"),\"gender\",\"level\")\n",
    "\n",
    "print(users_table)\n",
    "users_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: write users table to parquet files\n",
    "users_table.write.parquet(output_data + \"users\")\n",
    "\n",
    "#check this!\n",
    "#users_table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "#get_timestamp = udf()\n",
    "#df = \n",
    "    \n",
    "# create datetime column from original timestamp column\n",
    "#get_datetime = udf()\n",
    "#df = \n",
    "\n",
    "# Step 6: define ts format\n",
    "tsFormat = \"yyyy-MM-dd HH:MM:ss z\"\n",
    "# Step 7: convert ts to a timestamp format    \n",
    "time_table = df.withColumn('ts', to_timestamp(date_format((df.ts/1000).cast(dataType=TimestampType()), tsFormat), tsFormat))\n",
    "\n",
    "print(time_table)\n",
    "time_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: extract columns to create time table    \n",
    "time_table = time_table.select(col(\"ts\").alias(\"start_time\"),\n",
    "                                hour(col(\"ts\")).alias(\"hour\"),\n",
    "                                dayofmonth(col(\"ts\")).alias(\"day\"), \n",
    "                                weekofyear(col(\"ts\")).alias(\"week\"), \n",
    "                                month(col(\"ts\")).alias(\"month\"),\n",
    "                                year(col(\"ts\")).alias(\"year\"))\n",
    "\n",
    "print(time_table)\n",
    "time_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: write time table to parquet files partitioned by year and month\n",
    "time_table.write.partitionBy(\"year\", \"month\").parquet(output_data + \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: read in song data to use for songplays table\n",
    "song_df = spark.read.json(song_data)\n",
    "\n",
    "print(song_df)\n",
    "song_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table = song_df.join(df, song_df.artist_name==df.artist).withColumn(\"songplay_id\", monotonically_increasing_id()).withColumn('start_time', to_timestamp(date_format((col(\"ts\") /1000).cast(dataType=TimestampType()), tsFormat),tsFormat)).                             select(\"songplay_id\",\n",
    "           \"start_time\",                         \n",
    "           col(\"userId\").alias(\"user_id\"),\n",
    "           \"level\",\n",
    "           \"song_id\",\n",
    "           \"artist_id\",\n",
    "           col(\"sessionId\").alias(\"session_id\"),\n",
    "           col(\"artist_location\").alias(\"location\"),\n",
    "           \"userAgent\",\n",
    "           month(col(\"start_time\")).alias(\"month\"),\n",
    "           year(col(\"start_time\")).alias(\"year\"))\n",
    "\n",
    "print(songplays_table)\n",
    "songplays_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: write songplays table to parquet files partitioned by year and month\n",
    "songplays_table.write.partitionBy(\"year\", \"month\").parquet(output_data + \"songplays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = spark.read.parquet(\"s3a://project4-spark/songplays/*/*/*.parquet\")\n",
    "\n",
    "parquetFile.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
